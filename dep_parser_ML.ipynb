{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib as plt\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load('es_core_news_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract dependency tree features from a sentence\n",
    "def extract_dep_features(sentence):\n",
    "    # Parse the sentence\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Extract dependency features\n",
    "    dep_labels = []\n",
    "    dep_distances = []\n",
    "    for token in doc:\n",
    "        if token.dep_ != 'ROOT':\n",
    "            dep_labels.append(token.dep_)\n",
    "            dep_distances.append(abs(token.i - token.head.i))\n",
    "        \n",
    "    # Convert the features into a dictionary\n",
    "    dep_features = {\n",
    "        'nsubj': int('nsubj' in dep_labels),\n",
    "        'dobj': int('dobj' in dep_labels),\n",
    "        'dep_distance_1': int(1 in dep_distances),\n",
    "        'dep_distance_2': int(2 in dep_distances),\n",
    "        'dep_distance_3': int(3 in dep_distances),\n",
    "        # add more features as needed\n",
    "    }\n",
    "    \n",
    "    return dep_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labeled data\n",
    "df = pd.read_csv('C:\\\\Users\\\\Jerem\\\\Desktop\\\\counterfactuals.csv', encoding='utf8')\n",
    "\n",
    "df_label_0 = df[df['Label'] == 0]  # select only rows with label == 0\n",
    "df_label_0_sample = df_label_0.sample(n=653, random_state=42)  # randomly sample 653 rows\n",
    "\n",
    "df_label_1 = df[df['Label'] == 1]  # select only rows with label == 0\n",
    "\n",
    "sample_df = pd.concat([df_label_0_sample, df_label_1])\n",
    "\n",
    "X = sample_df['Text'].to_list()\n",
    "Y = sample_df['Label'].to_list()\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.19      0.31       133\n",
      "           1       0.54      0.98      0.69       129\n",
      "\n",
      "    accuracy                           0.58       262\n",
      "   macro avg       0.72      0.58      0.50       262\n",
      "weighted avg       0.72      0.58      0.50       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract features from the sentences\n",
    "all_features = [extract_dep_features(sentence) for sentence in X_train]\n",
    "\n",
    "# Convert the features into a matrix\n",
    "vec = DictVectorizer()\n",
    "X = vec.fit_transform(all_features)\n",
    "\n",
    "# Train a logistic regression model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X, y_train)\n",
    "\n",
    "# Predict the labels for new sentences\n",
    "\n",
    "new_features = [extract_dep_features(sentence) for sentence in X_test]\n",
    "new_X = vec.transform(new_features)\n",
    "new_labels = clf.predict(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5763358778625954\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.19      0.31       133\n",
      "           1       0.54      0.98      0.69       129\n",
      "\n",
      "    accuracy                           0.58       262\n",
      "   macro avg       0.72      0.58      0.50       262\n",
      "weighted avg       0.72      0.58      0.50       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, new_labels)\n",
    "print('Accuracy:', accuracy)\n",
    "print(classification_report(y_test, new_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4c35ab18d64beb73eb5bd0a413aff9d7b8895fdc9746fe77e18c17c088a689f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
